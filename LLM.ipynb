{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24631e76-3f54-4b42-8177-d051020bad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using a Pre-trained LLM for Text Generation ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dfdcb164fc4081be3f1436c7c089e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f1fdcec3a04e18a52ecefb3c32ed3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddd006c2eec431ebb4729fb1bc9481b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2c486c3226486eb1ffbfedf318b177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d427e712ec4cb9bc87da12d4c9ba5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f092e113e74428caad03118bb410bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff84f78da5fc4c339d0dfd5383761d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation pipeline loaded successfully using distilgpt2.\n",
      "\n",
      "--- Generating text with prompt: 'Once upon a time, in a land far, far away, there was a brave knight who' ---\n",
      "\n",
      "Generation 1 (max_new_tokens=50, num_return_sequences=1):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, there was a brave knight who stood.\n",
      "\n",
      "“He had a very brave knight out there. And if he were to lose that knight — he could have been so heroic just getting up to the king who had said that his entire world would follow him and turn on his\n",
      "\n",
      "Generation 2 (max_new_tokens=30, num_return_sequences=3):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sequence 1: Once upon a time, in a land far, far away, there was a brave knight who would only fight against another, and who was always afraid to do so. Every moment they were looking about the way forward and how difficult they were to\n",
      "  Sequence 2: Once upon a time, in a land far, far away, there was a brave knight who was all ready. ‑All through this great warrior, the one who led the mighty spear‑In all this great warrior, the one who\n",
      "  Sequence 3: Once upon a time, in a land far, far away, there was a brave knight who was brave enough to die, but then he did his own thing.\n",
      "\n",
      "Generation 3 (max_new_tokens=50, num_return_sequences=1, temperature=0.7 - more focused):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, there was a brave knight who would take the place of the King, to honor his fallen warriors. And as if he were the King, he would serve as a sort of knight to the people of the kingdom, at least to their friends.\n",
      "\n",
      "\n",
      "\n",
      "Now that the\n",
      "\n",
      "Generation 4 (max_new_tokens=50, num_return_sequences=1, temperature=1.2 - more creative):\n",
      "Once upon a time, in a land far, far away, there was a brave knight who did everything he'd done on his mind, on the side of a dead Prince as he made sure he had his orders met.\"\n",
      "\n",
      "\n",
      "The Knights were now at peace. All would come true. The Knights were still dead, but it was\n",
      "\n",
      "--- End of Text Generation Example ---\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have the transformers library installed\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"--- Using a Pre-trained LLM for Text Generation ---\")\n",
    "\n",
    "# 1. Create a text generation pipeline\n",
    "# We'll use \"distilgpt2\" as it's relatively small and fast for demonstration.\n",
    "# For more powerful generation, you'd use larger models like \"gpt2\", \"microsoft/DialoGPT-medium\", etc.\n",
    "try:\n",
    "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"Text generation pipeline loaded successfully using distilgpt2.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pipeline. Please ensure 'transformers' is installed, and you have an internet connection: {e}\")\n",
    "    exit() # Exit if pipeline fails to load\n",
    "\n",
    "# 2. Provide a prompt and generate text\n",
    "prompt = \"Once upon a time, in a land far, far away, there was a brave knight who\"\n",
    "\n",
    "print(f\"\\n--- Generating text with prompt: '{prompt}' ---\")\n",
    "\n",
    "# Generate a single sequence with a maximum of 50 new tokens\n",
    "print(\"\\nGeneration 1 (max_new_tokens=50, num_return_sequences=1):\")\n",
    "generated_text_1 = generator(prompt, max_new_tokens=50, num_return_sequences=1)\n",
    "print(generated_text_1[0]['generated_text'])\n",
    "\n",
    "# Generate multiple sequences to see variations\n",
    "print(\"\\nGeneration 2 (max_new_tokens=30, num_return_sequences=3):\")\n",
    "generated_texts_2 = generator(prompt, max_new_tokens=30, num_return_sequences=3)\n",
    "for i, gen in enumerate(generated_texts_2):\n",
    "    print(f\"  Sequence {i+1}: {gen['generated_text']}\")\n",
    "\n",
    "# 3. Optional: Experiment with temperature\n",
    "# Temperature controls randomness:\n",
    "#   Lower temperature (e.g., 0.7) makes output more deterministic/focused.\n",
    "#   Higher temperature (e.g., 1.2) makes output more creative/random.\n",
    "print(\"\\nGeneration 3 (max_new_tokens=50, num_return_sequences=1, temperature=0.7 - more focused):\")\n",
    "generated_text_temp_low = generator(prompt, max_new_tokens=50, num_return_sequences=1, temperature=0.7)\n",
    "print(generated_text_temp_low[0]['generated_text'])\n",
    "\n",
    "print(\"\\nGeneration 4 (max_new_tokens=50, num_return_sequences=1, temperature=1.2 - more creative):\")\n",
    "generated_text_temp_high = generator(prompt, max_new_tokens=50, num_return_sequences=1, temperature=1.2)\n",
    "print(generated_text_temp_high[0]['generated_text'])\n",
    "\n",
    "print(\"\\n--- End of Text Generation Example ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb37d06-7c04-4c8b-9d61-e0415968fe93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
